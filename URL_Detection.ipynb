{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "nzuegBzsBlCf"
      },
      "outputs": [],
      "source": [
        "!pip install -q streamlit pyngrok plotly pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat > feature_extraction.py <<'PY'\n",
        "\n",
        "import socket\n",
        "import ssl\n",
        "import hashlib\n",
        "import urllib.parse\n",
        "from datetime import datetime, timezone\n",
        "import whois\n",
        "from fuzzywuzzy import fuzz\n",
        "import geoip2.database\n",
        "import urllib.parse\n",
        "import sqlite3\n",
        "from datetime import datetime\n",
        "\n",
        "reader = geoip2.database.Reader(\"GeoLite2-City.mmdb\")\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 1️⃣ Resolve domain to IP\n",
        "# ---------------------------------------------\n",
        "def resolve_ip(domain):\n",
        "    try:\n",
        "        return socket.gethostbyname(domain)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to resolve {domain}: {e}\")\n",
        "        return None\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 2️⃣ WHOIS Information\n",
        "# ---------------------------------------------\n",
        "def get_whois_data(domain):\n",
        "    try:\n",
        "        w = whois.whois(domain)\n",
        "        created = w.creation_date\n",
        "        if isinstance(created, list):\n",
        "            created = created[0]\n",
        "        if created is not None:\n",
        "            # Normalize datetime\n",
        "            if created.tzinfo is not None:\n",
        "                created = created.replace(tzinfo=None)\n",
        "            domain_age_days = (datetime.utcnow() - created).days\n",
        "            creation_date = created.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        else:\n",
        "            domain_age_days = None\n",
        "            creation_date = None\n",
        "\n",
        "        return {\n",
        "            \"registrar\": w.registrar,\n",
        "            \"creation_date\": creation_date,\n",
        "            \"domain_age_days\": domain_age_days\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"WHOIS lookup failed for {domain}: {e}\")\n",
        "        return {\"registrar\": None, \"creation_date\": None, \"domain_age_days\": None}\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 3️⃣ Brand Similarity\n",
        "# ---------------------------------------------\n",
        "brand_list = [\"Google\", \"Facebook\", \"Microsoft\", \"Amazon\", \"PayPal\", \"Netflix\"]\n",
        "\n",
        "def compute_similarity(domain_name):\n",
        "    if not domain_name:\n",
        "        return 0\n",
        "    domain = domain_name.split('.')[0]  # remove .com, .net, etc.\n",
        "    try:\n",
        "        return max(fuzz.ratio(domain.lower(), brand.lower()) for brand in brand_list)\n",
        "    except Exception as e:\n",
        "        print(f\"Brand similarity failed: {e}\")\n",
        "        return 0\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 4️⃣ GeoIP Lookup\n",
        "# ---------------------------------------------\n",
        "def get_geoip_data(domain):\n",
        "    \"\"\"\n",
        "    Takes a domain name, resolves it to an IP, and returns GeoIP information.\n",
        "    Uses the GeoLite2-City.mmdb database.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ip = resolve_ip(domain)\n",
        "        if not ip:\n",
        "            return {\"ip\": None, \"country\": None, \"region\": None, \"city\": None}\n",
        "\n",
        "        response = reader.city(ip)\n",
        "        return {\n",
        "            \"ip\": ip,\n",
        "            \"country\": response.country.name,\n",
        "            \"region\": response.subdivisions.most_specific.name,\n",
        "            \"city\": response.city.name\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"GeoIP lookup failed for {domain}: {e}\")\n",
        "        return {\"ip\": None, \"country\": None, \"region\": None, \"city\": None}\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 5️⃣ ASN Info (placeholder — you already have it)\n",
        "# ---------------------------------------------\n",
        "def get_asn_info_from_ip(ip):\n",
        "    # Placeholder structure\n",
        "    try:\n",
        "        # Example only: replace with your own MaxMind ASN reader if available\n",
        "        return {\"asn\": \"AS15169\", \"asn_description\": \"GOOGLE LLC\"}\n",
        "    except Exception:\n",
        "        return {\"asn\": None, \"asn_description\": None}\n",
        "\n",
        "def get_asn_reputation(asn, asn_description):\n",
        "    # Simple logic example; replace with your scoring system\n",
        "    if asn_description and \"GOOGLE\" in asn_description.upper():\n",
        "        return \"Good\"\n",
        "    elif asn_description:\n",
        "        return \"Unknown\"\n",
        "    else:\n",
        "        return \"Bad\"\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 6️⃣ SSL Certificate Core Features\n",
        "# ---------------------------------------------\n",
        "def get_ssl_core_features(url, timeout=5):\n",
        "    \"\"\"\n",
        "    Returns dict with SSL certificate features:\n",
        "    fingerprint_sha256, issuer, subject, not_valid_before, not_valid_after, error\n",
        "    \"\"\"\n",
        "    try:\n",
        "        parsed = urllib.parse.urlparse(url)\n",
        "        hostname = parsed.hostname or url\n",
        "        scheme = parsed.scheme or \"http\"\n",
        "        port = parsed.port or (443 if scheme == \"https\" else 80)\n",
        "\n",
        "        if scheme == \"http\":\n",
        "            return {\n",
        "                \"fingerprint_sha256\": None,\n",
        "                \"issuer\": None,\n",
        "                \"subject\": None,\n",
        "                \"not_valid_before\": None,\n",
        "                \"not_valid_after\": None,\n",
        "                \"error\": \"no_ssl_for_http\"\n",
        "            }\n",
        "\n",
        "        ctx = ssl.create_default_context()\n",
        "        with socket.create_connection((hostname, port), timeout=timeout) as sock:\n",
        "            with ctx.wrap_socket(sock, server_hostname=hostname) as ssock:\n",
        "                der_cert = ssock.getpeercert(binary_form=True)\n",
        "                cert_info = ssock.getpeercert()\n",
        "\n",
        "                fingerprint = hashlib.sha256(der_cert).hexdigest().upper()\n",
        "\n",
        "                issuer = \", \".join([f\"{k}={v}\" for part in cert_info.get(\"issuer\", []) for k, v in part]) or None\n",
        "                subject = \", \".join([f\"{k}={v}\" for part in cert_info.get(\"subject\", []) for k, v in part]) or None\n",
        "\n",
        "                nb = na = None\n",
        "                try:\n",
        "                    nb = datetime.strptime(cert_info[\"notBefore\"], \"%b %d %H:%M:%S %Y %Z\").isoformat()\n",
        "                    na = datetime.strptime(cert_info[\"notAfter\"], \"%b %d %H:%M:%S %Y %Z\").isoformat()\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "                return {\n",
        "                    \"fingerprint_sha256\": fingerprint,\n",
        "                    \"issuer\": issuer,\n",
        "                    \"subject\": subject,\n",
        "                    \"not_valid_before\": nb,\n",
        "                    \"not_valid_after\": na,\n",
        "                    \"error\": None\n",
        "                }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"fingerprint_sha256\": None,\n",
        "            \"issuer\": None,\n",
        "            \"subject\": None,\n",
        "            \"not_valid_before\": None,\n",
        "            \"not_valid_after\": None,\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "# ---------------------------------------------\n",
        "# 7️⃣ Risk Score Calculation (based on your DB logic)\n",
        "# ---------------------------------------------\n",
        "def calculate_risk_score(domain, domain_age_days, brand_similarity, fingerprint_sha256, all_domains):\n",
        "    \"\"\"\n",
        "    Compute risk score:\n",
        "    - feed frequency (log scale)\n",
        "    - domain age (younger = riskier)\n",
        "    - SSL mismatch\n",
        "    - brand similarity\n",
        "    \"\"\"\n",
        "    try:\n",
        "        feed_freq = all_domains.count(domain) if domain else 1\n",
        "        total_domains = max(all_domains.count(d) for d in all_domains)\n",
        "        feed_score = np.log1p(feed_freq) / np.log1p(total_domains) * 25 if total_domains > 0 else 0\n",
        "    except Exception:\n",
        "        feed_score = 0\n",
        "\n",
        "    try:\n",
        "        domain_age_days = float(domain_age_days or 0)\n",
        "    except:\n",
        "        domain_age_days = 0\n",
        "    max_age = 365 * 5\n",
        "    age_score = (1 - (domain_age_days / max_age)) * 25\n",
        "    age_score = np.clip(age_score, 0, 25)\n",
        "\n",
        "    ssl_score = 25 if fingerprint_sha256 is None or str(fingerprint_sha256).strip() == \"\" else 5\n",
        "\n",
        "    try:\n",
        "        sim = float(brand_similarity or 0)\n",
        "        if sim > 1:\n",
        "            sim /= 100.0\n",
        "        sim = np.clip(sim, 0, 1)\n",
        "    except Exception:\n",
        "        sim = 0\n",
        "    brand_score = sim * 25\n",
        "\n",
        "    total = feed_score + age_score + ssl_score + brand_score\n",
        "    total = np.clip(total, 0, 100)\n",
        "\n",
        "    if total >= 70:\n",
        "        level = \"High\"\n",
        "    elif total >= 40:\n",
        "        level = \"Medium\"\n",
        "    else:\n",
        "        level = \"Low\"\n",
        "\n",
        "    return round(total, 2), level\n",
        "\n",
        "# =====================================================\n",
        "# 2️⃣ Helper function — check if URL already exists in DB\n",
        "# =====================================================\n",
        "def url_exists_in_db(url, db_path):\n",
        "    db_path = \"/content/phishing_dataset.db\"\n",
        "    \"\"\"Check if the URL already exists in the database and return its record.\"\"\"\n",
        "\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    cursor.execute(\"\"\"\n",
        "        SELECT url_id, url, domain, date_added, verified, url_status, label,\n",
        "               whois_registrar, creation_date, domain_age_days, brand_similarity,\n",
        "               ip, country, region, city,\n",
        "               fingerprint_sha256, issuer, subject, not_valid_before, not_valid_after,\n",
        "               error, risk_score, risk_level\n",
        "        FROM urls\n",
        "        WHERE url = ?;\n",
        "    \"\"\", (url,))\n",
        "    row = cursor.fetchone()\n",
        "    columns = [desc[0] for desc in cursor.description]\n",
        "    conn.close()\n",
        "\n",
        "    if row:\n",
        "        return dict(zip(columns, row))\n",
        "    return None\n",
        "\n",
        "PY"
      ],
      "metadata": {
        "id": "5vjAAy-xRH2v"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat > phishing_detector.py <<'PY'\n",
        "# phishing_detector.py\n",
        "from urllib.parse import urlparse\n",
        "from feature_extraction import (\n",
        "    get_whois_data,\n",
        "    compute_similarity,\n",
        "    get_geoip_data,\n",
        "    get_ssl_core_features,\n",
        "    get_asn_info_from_ip,\n",
        "    get_asn_reputation,\n",
        ")\n",
        "import numpy as np\n",
        "\n",
        "def enrich_url(url):\n",
        "    \"\"\"Gather real enrichment features from the actual extraction functions\"\"\"\n",
        "    parsed = urlparse(url if url.startswith(\"http\") else \"http://\" + url)\n",
        "    domain = parsed.netloc or parsed.path\n",
        "\n",
        "    # WHOIS\n",
        "    whois_info = get_whois_data(domain)\n",
        "    domain_age_days = whois_info.get(\"domain_age_days\") or 0\n",
        "    registrar = whois_info.get(\"registrar\")\n",
        "\n",
        "    # Brand similarity\n",
        "    brand_similarity = compute_similarity(domain)\n",
        "\n",
        "    # GeoIP\n",
        "    geo_data = get_geoip_data(domain)\n",
        "    ip = geo_data.get(\"ip\")\n",
        "    country = geo_data.get(\"country\")\n",
        "    region = geo_data.get(\"region\")\n",
        "    city = geo_data.get(\"city\")\n",
        "\n",
        "    # ASN\n",
        "    asn_info = get_asn_info_from_ip(ip) if ip else {\"asn\": None, \"asn_description\": None}\n",
        "    asn = asn_info.get(\"asn\")\n",
        "    asn_desc = asn_info.get(\"asn_description\")\n",
        "    asn_reputation = get_asn_reputation(asn, asn_desc)\n",
        "\n",
        "    # SSL\n",
        "    ssl_info = get_ssl_core_features(url)\n",
        "    ssl_valid = \"No\" if ssl_info.get(\"error\") else \"Yes\"\n",
        "\n",
        "    enrich = {\n",
        "        \"domain\": domain,\n",
        "        \"domain_age_days\": domain_age_days,\n",
        "        \"ssl_valid\": ssl_valid,\n",
        "        \"brand_similarity\": brand_similarity,\n",
        "        \"geo_location\": country or \"Unknown\",\n",
        "        \"asn\": asn or \"Unknown\",\n",
        "        \"asn_reputation\": asn_reputation,\n",
        "        \"registrar\": registrar,\n",
        "        \"city\": city,\n",
        "        \"region\": region\n",
        "    }\n",
        "    return enrich\n",
        "\n",
        "\n",
        "def score_from_enrichment(enrich):\n",
        "    \"\"\"Compute risk score using real features\"\"\"\n",
        "    score = 0\n",
        "    if enrich[\"ssl_valid\"] == \"No\":\n",
        "        score += 30\n",
        "    if enrich[\"domain_age_days\"] and enrich[\"domain_age_days\"] < 60:\n",
        "        score += 25\n",
        "    if enrich[\"brand_similarity\"] and enrich[\"brand_similarity\"] > 70:\n",
        "        score += 20\n",
        "    if enrich[\"asn_reputation\"] == \"Bad\":\n",
        "        score += 15\n",
        "    score = np.clip(score, 0, 100)\n",
        "    return float(score)\n",
        "\n",
        "\n",
        "def analyze_url(url):\n",
        "    \"\"\"Main function: analyze URL and return phishing risk\"\"\"\n",
        "    if not url:\n",
        "        raise ValueError(\"Empty URL provided\")\n",
        "\n",
        "    enrich = enrich_url(url)\n",
        "    score = score_from_enrichment(enrich)\n",
        "    verdict = \"Phishing\" if score >= 60 else \"Benign\"\n",
        "\n",
        "    result = {\n",
        "        \"url\": url,\n",
        "        \"score\": score,\n",
        "        \"verdict\": verdict,\n",
        "        \"details\": enrich\n",
        "    }\n",
        "    return result\n",
        "PY\n"
      ],
      "metadata": {
        "id": "BfpnZGQbBzNk"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-whois fuzzywuzzy[speedup] geoip2 plotly streamlit pandas numpy thefuzz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxOqc8DIWvQV",
        "outputId": "eaf08c50-131f-4d70-c9b5-3ef8b0b242b3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-whois in /usr/local/lib/python3.12/dist-packages (0.9.6)\n",
            "Requirement already satisfied: geoip2 in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.50.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: thefuzz in /usr/local/lib/python3.12/dist-packages (0.22.1)\n",
            "Requirement already satisfied: fuzzywuzzy[speedup] in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from python-whois) (2.9.0.post0)\n",
            "Requirement already satisfied: python-levenshtein>=0.12 in /usr/local/lib/python3.12/dist-packages (from fuzzywuzzy[speedup]) (0.27.1)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from geoip2) (3.13.0)\n",
            "Requirement already satisfied: maxminddb<3.0.0,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from geoip2) (2.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.12/dist-packages (from geoip2) (2.32.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly) (25.0)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.0)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from thefuzz) (3.14.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (1.22.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.7.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil->python-whois) (1.17.0)\n",
            "Requirement already satisfied: Levenshtein==0.27.1 in /usr/local/lib/python3.12/dist-packages (from python-levenshtein>=0.12->fuzzywuzzy[speedup]) (0.27.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.24.0->geoip2) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.24.0->geoip2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.24.0->geoip2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.24.0->geoip2) (2025.10.5)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O GeoLite2-City.mmdb \"https://github.com/P3TERX/GeoLite.mmdb/raw/download/GeoLite2-City.mmdb\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKpQcKXHYEMl",
        "outputId": "1adb40cb-2daf-4cb3-b2fd-2fc5bf919aed"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-16 14:43:21--  https://github.com/P3TERX/GeoLite.mmdb/raw/download/GeoLite2-City.mmdb\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/P3TERX/GeoLite.mmdb/download/GeoLite2-City.mmdb [following]\n",
            "--2025-10-16 14:43:22--  https://raw.githubusercontent.com/P3TERX/GeoLite.mmdb/download/GeoLite2-City.mmdb\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 62557403 (60M) [application/octet-stream]\n",
            "Saving to: ‘GeoLite2-City.mmdb’\n",
            "\n",
            "GeoLite2-City.mmdb  100%[===================>]  59.66M  83.4MB/s    in 0.7s    \n",
            "\n",
            "2025-10-16 14:43:22 (83.4 MB/s) - ‘GeoLite2-City.mmdb’ saved [62557403/62557403]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat > app.py <<'PY'\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from phishing_detector import analyze_url\n",
        "\n",
        "st.set_page_config(page_title=\"Phishing Detection Dashboard\", layout=\"wide\")\n",
        "st.title(\"Phishing Detection Dashboard\")\n",
        "st.markdown(\"Enter a URL and get a phishing risk score and details.\")\n",
        "\n",
        "with st.expander(\"How to use\"):\n",
        "    st.write(\"Paste a URL (e.g. https://example.com) then click Analyze URL.\")\n",
        "\n",
        "url = st.text_input(\"🔗 Enter a URL to analyze:\")\n",
        "\n",
        "if st.button(\"Analyze URL\"):\n",
        "    if url.strip():\n",
        "        try:\n",
        "            result = analyze_url(url.strip())\n",
        "            col1, col2 = st.columns([1,1])\n",
        "            with col1:\n",
        "                st.metric(\"Risk Score\", f\"{result['score']} / 100\")\n",
        "            with col2:\n",
        "                color = \"red\" if result['verdict'] == \"Phishing\" else \"green\"\n",
        "                st.markdown(f\"### Verdict: <span style='color:{color}'>{result['verdict']}</span>\", unsafe_allow_html=True)\n",
        "\n",
        "            st.subheader(\"Details\")\n",
        "            st.json(result[\"details\"])\n",
        "\n",
        "            df = pd.DataFrame([result[\"details\"]])\n",
        "            st.table(df.T.rename(columns={0:\"Value\"}))\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error analyzing URL: {e}\")\n",
        "    else:\n",
        "        st.warning(\"Please enter a valid URL first!\")\n",
        "\n",
        "st.subheader(\"Sample: Top Targeted Brands (Example Data)\")\n",
        "df = pd.DataFrame({\n",
        "    \"Brand\": [\"PayPal\", \"Google\", \"Microsoft\", \"Netflix\"],\n",
        "    \"Phishing Attempts\": [23, 17, 31, 9]\n",
        "})\n",
        "fig = px.bar(df, x=\"Brand\", y=\"Phishing Attempts\", title=\"Top Targeted Brands\")\n",
        "st.plotly_chart(fig, use_container_width=True)\n",
        "PY\n"
      ],
      "metadata": {
        "id": "SeUeYcLLB7Ks"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 349GdIdCVwNzMfnr6VflsBxNbBX_6atP5FyipTgnDcgdiQnT3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbQZdAiSDMcj",
        "outputId": "e3d52c04-4267-45d7-9e4a-8783711dc832"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import subprocess, time, os\n",
        "\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Public URL:\", public_url)\n",
        "\n",
        "cmd = \"nohup streamlit run app.py --server.port 8501 > streamlit.log 2>&1 &\"\n",
        "print(\"Starting Streamlit...\")\n",
        "subprocess.Popen(cmd, shell=True)\n",
        "time.sleep(3)\n",
        "print(\"Streamlit started. Check the public URL above.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49n3O4XmB-vl",
        "outputId": "36739ee5-acb1-407d-d446-76f2db18a644"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://maidenly-oneirocritically-adrianna.ngrok-free.dev\" -> \"http://localhost:8501\"\n",
            "Starting Streamlit...\n",
            "Streamlit started. Check the public URL above.\n"
          ]
        }
      ]
    }
  ]
}